{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6386288",
   "metadata": {
    "id": "a6386288"
   },
   "source": [
    "\n",
    "# MNIST Digit Classification and FGSM Attack\n",
    "\n",
    "The following code will guide you through training a simple Multi-Layer Perceptron (MLP) model using Keras on the MNIST dataset. We will then implement the Fast Gradient Sign Method (FGSM) attack to generate adversarial examples and observe how they affect model performance.\n",
    "\n",
    "## About MNIST Dataset\n",
    "MNIST (Modified National Institute of Standards and Technology) is a dataset of 70,000 grayscale images of handwritten digits (0-9), each of size 28x28 pixels. The dataset is commonly used for training and testing machine learning models for digit recognition. It consists of:\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "- Labels corresponding to digits from 0 to 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34673d1f",
   "metadata": {
    "id": "34673d1f"
   },
   "source": [
    "## Step 1: Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dededee8",
   "metadata": {
    "id": "dededee8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 16:07:27.267353: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c8d25",
   "metadata": {
    "id": "cb3c8d25"
   },
   "source": [
    "## Step 2: Loading and Preprocessing the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e91b15",
   "metadata": {
    "id": "a2e91b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the MNIST dataset from Keras datasets\n",
    "# This returns two tuples:\n",
    "# - (x_train, y_train): Training images and their corresponding labels\n",
    "# - (x_test, y_test): Testing images and their corresponding labels\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to the range [0,1] by dividing by 255\n",
    "# This helps in faster and more stable training of the neural network\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Reshape the images from 28x28 (2D) to a single 784-dimensional vector (1D)\n",
    "# -1 automatically infers the batch size, keeping all images\n",
    "# This transformation is necessary for feeding into a fully connected neural network\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "# Convert the labels from integer format (0-9) to one-hot encoded format\n",
    "# Example: Label \"3\" â†’ [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "# This is required for categorical classification in neural networks\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc790d2",
   "metadata": {
    "id": "5dc790d2"
   },
   "source": [
    "## Step 3: Building the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8712fa84",
   "metadata": {
    "id": "8712fa84"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Defining a Multi-Layer Perceptron (MLP) model for MNIST digit classification.\n",
    "# We will use Keras' Sequential API to stack layers in a simple feedforward manner.\n",
    "\n",
    "# The model consists of three layers:\n",
    "# 1. **First Hidden Layer (512 neurons, ReLU activation)**:\n",
    "#    - The input layer expects images of size 28x28, which are flattened into a 784-dimensional vector.\n",
    "#    - This layer has 512 neurons to learn complex features from the digit images.\n",
    "#    - The activation function used is **ReLU (Rectified Linear Unit)**, which helps introduce non-linearity and prevents the vanishing gradient problem.\n",
    "\n",
    "# 2. **Second Hidden Layer (256 neurons, ReLU activation)**:\n",
    "#    - A fully connected layer with 256 neurons.\n",
    "#    - Also uses **ReLU activation** to maintain non-linearity.\n",
    "\n",
    "# 3. **Output Layer (10 neurons, Softmax activation)**:\n",
    "#    - The final layer has **10 neurons**, corresponding to the 10 digit classes (0-9).\n",
    "#    - The **Softmax activation function** is used to output probability distributions over the 10 classes.\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(28*28,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14e4ac",
   "metadata": {
    "id": "2e14e4ac"
   },
   "source": [
    "## Step 4: Compiling and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c17e5",
   "metadata": {
    "id": "e59c17e5"
   },
   "outputs": [],
   "source": [
    "# Compiling the model:\n",
    "# - Optimizer: 'adam' (adaptive learning rate for efficient training)\n",
    "# - Loss: 'categorical_crossentropy' (suitable for multi-class classification)\n",
    "# - Metric: 'accuracy' (measures correct predictions)\n",
    "\n",
    "# Training the model:\n",
    "# - Epochs: 10 (number of times the model sees the dataset)\n",
    "# - Batch size: 128 (updates weights after every 128 samples)\n",
    "# - Validation data: (x_test, y_test) (evaluates model performance on unseen data)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956abd9",
   "metadata": {
    "id": "1956abd9"
   },
   "source": [
    "## Step 5: Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c61237",
   "metadata": {
    "id": "04c61237"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test) # Evaluate the model on the test data\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")  # Print the accuracy value with 4 decimal places\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e9f48",
   "metadata": {
    "id": "922e9f48"
   },
   "source": [
    "## Step 6: Implementing the FGSM Attack\n",
    "We will use Fast Gradient Sign Method (FGSM) to create adversarial examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0be0547",
   "metadata": {
    "id": "e0be0547"
   },
   "outputs": [],
   "source": [
    "# The function fgsm_attack generates adversarial examples using the Fast Gradient Sign Method (FGSM).\n",
    "# The FGSM method perturbs the input image in the direction of the gradient of the loss with respect to the image,\n",
    "# scaled by a small value called epsilon. This creates an adversarial example that can deceive the model.\n",
    "\n",
    "# Parameters:\n",
    "# - image: The original input image that we want to perturb.\n",
    "# - epsilon: A small scalar that controls the magnitude of the perturbation. A larger epsilon means a stronger perturbation.\n",
    "# - gradient: The gradient of the loss with respect to the image. This is typically computed during backpropagation.\n",
    "\n",
    "# The function performs the following:\n",
    "# 1. Calculates the perturbation using the sign of the gradient, scaled by epsilon.\n",
    "# 2. Adds this perturbation to the original image to generate the adversarial image.\n",
    "# 3. Clips the resulting image to ensure the pixel values stay within the valid range [0, 1].\n",
    "# 4. Returns the adversarial image and the perturbation used\n",
    "\n",
    "def fgsm_attack(image, epsilon, gradient):\n",
    "    perturbation = epsilon * np.sign(gradient)\n",
    "    adversarial_image = image + perturbation\n",
    "    adversarial_image = np.clip(adversarial_image, 0, 1)\n",
    "    return adversarial_image, perturbation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d1d0e9",
   "metadata": {
    "id": "66d1d0e9"
   },
   "source": [
    "## Step 7: Generating Adversarial Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc848cfb",
   "metadata": {
    "id": "fc848cfb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This function creates adversarial examples using the Fast Gradient Sign Method (FGSM).\n",
    "# It calculates the gradient of the loss with respect to the input image and generates an adversarial version of the image\n",
    "# by applying a perturbation. The goal is to fool the model by slightly altering the input image in a way that misleads\n",
    "# the model into making incorrect predictions.\n",
    "\n",
    "# Parameters:\n",
    "# - model: The trained neural network model to evaluate on the adversarial examples.\n",
    "# - images: The original input images that will be perturbed to create adversarial examples.\n",
    "# - labels: The true labels corresponding to the images.\n",
    "# - epsilon: A small scalar that controls the strength of the perturbation (default is 0.1).\n",
    "\n",
    "# The function does the following:\n",
    "# 1. Converts the input images to a tensor, as TensorFlow models require tensor inputs.\n",
    "# 2. Uses a GradientTape to track operations on the image tensor and compute the loss.\n",
    "# 3. Computes the gradients of the loss with respect to the input images.\n",
    "# 4. Uses the fgsm_attack function to create adversarial examples by perturbing the input images based on the computed gradients.\n",
    "# 5. Returns the adversarial examples and the perturbations applied.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def create_adversarial_examples(model, images, labels, epsilon=0.1):\n",
    "    # Convert the images to a tensor for TensorFlow model compatibility\n",
    "    images_tensor = tf.convert_to_tensor(images)\n",
    "\n",
    "    # Track the gradients of the loss with respect to the input images\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images_tensor)  # Ensure the tape tracks the images tensor\n",
    "        predictions = model(images_tensor)  # Get the model's predictions for the images\n",
    "        loss = loss_object(labels, predictions)  # Compute the loss based on the true labels and model predictions\n",
    "\n",
    "    # Compute the gradients of the loss with respect to the input images\n",
    "    gradients = tape.gradient(loss, images_tensor)\n",
    "\n",
    "    # Generate adversarial examples by applying the perturbation based on the gradients\n",
    "    adversarial_examples, perturbations = fgsm_attack(images, epsilon, gradients.numpy())\n",
    "\n",
    "    # Return the adversarial examples and the perturbations\n",
    "    return adversarial_examples, perturbations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0d9717",
   "metadata": {
    "id": "3a0d9717"
   },
   "source": [
    "## Step 8: Selecting Sample Images and Creating Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59a2cd88",
   "metadata": {
    "id": "59a2cd88"
   },
   "outputs": [],
   "source": [
    "\n",
    "num_samples = 10  # Number of samples to select\n",
    "x_sample = x_test[:num_samples]  # Select first 10 test images\n",
    "y_sample = y_test[:num_samples]  # Select corresponding labels\n",
    "\n",
    "x_adversarial, noise = create_adversarial_examples(model, x_sample, y_sample, epsilon=0.2)  # Generate adversarial examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5d4ef",
   "metadata": {
    "id": "c5a5d4ef"
   },
   "source": [
    "## Step 9: Making Predictions on Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232db3c",
   "metadata": {
    "id": "a232db3c"
   },
   "outputs": [],
   "source": [
    "\n",
    "predictions = np.argmax(model.predict(x_adversarial), axis=1)  # Get model predictions for adversarial examples\n",
    "true_labels = np.argmax(y_sample, axis=1)  # Convert one-hot encoded labels to class indices\n",
    "\n",
    "loss_adv, accuracy_adv = model.evaluate(x_adversarial, y_sample)  # Evaluate model on adversarial examples\n",
    "print(f\"Test Accuracy after FGSM attack: {accuracy_adv:.4f}\")  # Print accuracy after adversarial attack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a7e22",
   "metadata": {
    "id": "7f5a7e22"
   },
   "source": [
    "## Step 10: Visualizing Original, Noise, and Adversarial Images Side by Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b356bb",
   "metadata": {
    "id": "e2b356bb"
   },
   "outputs": [],
   "source": [
    "# Create a 3x10 grid of subplots to display images\n",
    "# Loop through each sample to display the original image, noise, and adversarial image\n",
    "\n",
    "fig, axes = plt.subplots(3, 10, figsize=(15, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    # Original image\n",
    "    axes[0, i].imshow(x_sample[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title(f\"Orig: {true_labels[i]}\")\n",
    "\n",
    "    # Noise\n",
    "    axes[1, i].imshow(noise[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title(\"Noise\")\n",
    "\n",
    "    # Adversarial image\n",
    "    axes[2, i].imshow(x_adversarial[i].reshape(28, 28), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "    axes[2, i].set_title(f\"Adv: {predictions[i]}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c3383",
   "metadata": {
    "id": "355c3383"
   },
   "source": [
    "## Step 11: Identifying Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25f6ffae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25f6ffae",
    "outputId": "b918844e-ca09-473a-e732-196102b5e8fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly classified samples after FGSM attack:\n",
      "True Label: 7, Predicted: 3\n",
      "True Label: 1, Predicted: 2\n",
      "True Label: 0, Predicted: 3\n",
      "True Label: 4, Predicted: 0\n",
      "True Label: 1, Predicted: 7\n",
      "True Label: 4, Predicted: 2\n",
      "True Label: 9, Predicted: 3\n",
      "True Label: 5, Predicted: 6\n",
      "True Label: 9, Predicted: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the indices of incorrectly classified samples after the FGSM attack\n",
    "# Print the true and predicted labels for incorrectly classified samples\n",
    "\n",
    "incorrect_indices = np.where(predictions != true_labels)[0]  # Get indices of misclassified samples\n",
    "print(\"Incorrectly classified samples after FGSM attack:\")  # Print message for misclassified samples\n",
    "\n",
    "# Loop through each misclassified sample and print its true and predicted labels\n",
    "for idx in incorrect_indices:\n",
    "    print(f\"True Label: {true_labels[idx]}, Predicted: {predictions[idx]}\")  # Display true vs predicted labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VMFc8eNpcLTZ",
   "metadata": {
    "id": "VMFc8eNpcLTZ"
   },
   "source": [
    "Based on the above code, perform the following:\n",
    "\n",
    "**Adversarial Example Generation**\n",
    "1. Use the create_adversarial_examples function to generate adversarial examples\n",
    "for the given dataset (x_test, y_test). You will need to experiment with various values of epsilon (e.g., 0.1, 0.2, 0.3, 0.5, 1.0).\n",
    "\n",
    "2. For each epsilon value, generate adversarial examples for a subset of x_test (you can choose 10 samples).\n",
    "\n",
    "3. For each epsilon, store the adversarial examples and perturbations (noise).\n",
    "\n",
    "**Model Evaluatation**\n",
    "1. Evaluate the model on the generated adversarial examples for each epsilon value.\n",
    "\n",
    "2. For each evaluation, compute the loss and accuracy.\n",
    "\n",
    "3. Print the accuracy of the model on adversarial examples for each epsilon value.\n",
    "\n",
    "4. Record how the accuracy decreases as epsilon increases (i.e., the strength of the perturbation).\n",
    "\n",
    "**Visualization**:\n",
    "\n",
    "For each epsilon value, plot the following:\n",
    "1. The original images from the selected test samples.\n",
    "\n",
    "2. The perturbation (noise) applied to each image to generate the adversarial examples.\n",
    "\n",
    "3. The adversarial examples themselves.\n",
    "\n",
    "Label the images accordingly (e.g., Original, Noise, and Adversarial).\n",
    "\n",
    "\n",
    "# Important: Put the following graph in your Lab-Logbook\n",
    "**Graphing the Results:**\n",
    "\n",
    "\n",
    "1. Plot a graph showing the model's accuracy for each epsilon value. The x-axis should represent the epsilon values, and the y-axis should represent the accuracy of the model on the adversarial examples.\n",
    "\n",
    "2. Discuss how the accuracy changes as the strength of the adversarial perturbation increases. What does this tell you about the model's robustness to adversarial attacks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xHxNSjhNipwe",
   "metadata": {
    "id": "xHxNSjhNipwe"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "# Your code to perform the above task.\n",
    "# You can split your code into multiple cells.\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2VeROyBBexUs",
   "metadata": {
    "id": "2VeROyBBexUs"
   },
   "source": [
    "In the next assignment, you will perform training data poisoning. Remember, x_train and y_train contain the training data. Your task is to poison the training data in a way that impacts the model's performance.\n",
    "\n",
    "You will alter the labels in y_train by assigning incorrect labels to affect the model's accuracy. For example, you can set all values in y_train to a single label, chosen from 1 to 10. Alternatively, you can flip labels, such as setting label 2 to 1, and label 3 to 2, to degrade the modelâ€™s performance.\n",
    "\n",
    "Feel free to perform any label manipulation of your choice in the training data, and observe the impact on the modelâ€™s accuracy.\n",
    "\n",
    "# Important: In your lab logbook, provide the following:\n",
    "\n",
    "The model accuracy before data poisoning.\n",
    "The model accuracy after data poisoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eWPVkN9Iir5v",
   "metadata": {
    "id": "eWPVkN9Iir5v"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# Perform Data poisoning on the train data, and then train and test your model\n",
    "# You can split your code into multiple cells\n",
    "# Hint: y_train[:] = 1  # Set all labels to 1\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kTEPdk5Mjdru",
   "metadata": {
    "id": "kTEPdk5Mjdru"
   },
   "source": [
    "**Optional Part**\n",
    "\n",
    "In this task, similar to the FGSM attack, you will implement a PGD (Projected Gradient Descent) attack on the AI model. The PGD attack is an iterative adversarial attack that refines the adversarial perturbation step-by-step to create stronger adversarial examples.\n",
    "\n",
    "You can use the following libraries to implement the PGD attack. Make sure to experiment with different values of the attack parameters (e.g., number of iterations, step size, and epsilon) to observe how the attack affects the modelâ€™s performance.\n",
    "\n",
    "\n",
    "\n",
    "https://github.com/Trusted-AI/adversarial-robustness-toolbox"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
